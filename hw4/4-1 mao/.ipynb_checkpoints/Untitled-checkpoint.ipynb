{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, scipy, scipy.misc\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing image stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(o,image_size=[80,80]):\n",
    "    \"\"\"    \n",
    "    Input: \n",
    "    RGB image: np.array\n",
    "        RGB screen of game, shape: (210, 160, 3)\n",
    "    Default return: np.array \n",
    "        Grayscale image, shape: (80, 80, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    y = o.astype(np.uint8)\n",
    "    resized = scipy.misc.imresize(y, image_size) # (80,80,3)\n",
    "    # return np.expand_dims(resized.astype(np.float32),axis=2)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160)\n",
      "(80, 80)\n",
      "(1, 80, 80)\n"
     ]
    }
   ],
   "source": [
    "data = 0.2126 * data[:, :, 0] + 0.7152 * data[:, :, 1] + 0.0722 * data[:, :, 2]\n",
    "print(data.shape)\n",
    "resized = cv2.resize(data, dsize=(80, 80), interpolation=cv2.INTER_CUBIC)\n",
    "print(resized.shape)\n",
    "resized = np.expand_dims(resized,axis=0)\n",
    "print(resized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 80, 1)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    data, _, _, _ = env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = env.reset()\n",
    "# data = prepro(data)\n",
    "data = 0.2126 * data[:, :, 0] + 0.7152 * data[:, :, 1] + 0.0722 * data[:, :, 2]\n",
    "data = data[30:,:]\n",
    "resized = cv2.resize(data, dsize=(80, 80), interpolation=cv2.INTER_CUBIC)\n",
    "print(resized.shape)\n",
    "plt.gray()\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(resized, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Variable(torch.Tensor(np.expand_dims(resized,axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 78, 78])\n",
      "torch.Size([1, 32, 76, 76])\n"
     ]
    }
   ],
   "source": [
    "x = conv1(img)\n",
    "print(x.shape)\n",
    "y = conv2(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, gamma=0.99, lr=1e-4, rmsprop_decay=0.99):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "#         self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3) # (batch_size, 16, 208, 158)\n",
    "#         self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3) # (batch_size, 32, 206, 156)\n",
    "#         self.fc1 = torch.nn.Linear(32 * 76 * 76, 64)\n",
    "#         self.fc2 = torch.nn.Linear(64, 32)\n",
    "#         self.fc3 = torch.nn.Linear(32, 3) # 6 actions to choose from, only taking 3 here\n",
    "#         # known actions: 0(no move), 2(up), 3(down)\n",
    "\n",
    "        self.fc4 = torch.nn.Linear(80*80, 256)\n",
    "        self.fc5 = torch.nn.Linear(256, 256)\n",
    "        self.fc6 = torch.nn.Linear(256, 3)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.rmsprop_decay = rmsprop_decay\n",
    "        self.random_action_episodes = 0\n",
    "        \n",
    "        self.output2action = {0: 0, 1: 2, 2: 3}\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, x): # x: np.array (1, 80, 80)\n",
    "\n",
    "        x = Variable(torch.Tensor(x))\n",
    "        if torch.cuda.is_available():\n",
    "             x = x.cuda()\n",
    "        x = x.view(-1, 80*80)\n",
    "        x = self.fc4(x) # TODO: add batch norm?\n",
    "        x = torch.nn.functional.selu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = torch.nn.functional.selu(x)\n",
    "        x = self.fc6(x)\n",
    "\n",
    "\n",
    "        action_probs = torch.nn.functional.softmax(x, dim=1)\n",
    "        return action_probs # (batch_size, 6)\n",
    "\n",
    "    def reset(self):\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \n",
      "(0 ,0 ,.,.) = \n",
      "  0.2586  0.0846 -0.2774\n",
      "  0.2388 -0.0790  0.2701\n",
      " -0.1732  0.0489  0.2785\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.2182 -0.1425 -0.0518\n",
      " -0.0398  0.0854  0.1915\n",
      " -0.3303  0.1431  0.2565\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.1048  0.1597  0.1135\n",
      "  0.2579  0.1314  0.2567\n",
      "  0.2295 -0.0587 -0.1331\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.2432  0.0443  0.0329\n",
      "  0.2316  0.2493  0.1490\n",
      " -0.3180  0.0900  0.1506\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.1472 -0.3124 -0.2374\n",
      " -0.0486  0.3094 -0.2794\n",
      "  0.0868 -0.2914  0.2097\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1925 -0.2678  0.2011\n",
      "  0.2991 -0.0440  0.1304\n",
      "  0.2619 -0.1084  0.0368\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.2228  0.0782 -0.0590\n",
      "  0.0921  0.1476  0.2049\n",
      "  0.0861  0.0727  0.2415\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.2416 -0.1896 -0.0700\n",
      "  0.0671  0.2729 -0.2651\n",
      " -0.0382 -0.2479  0.2463\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0256  0.0123 -0.1102\n",
      " -0.1485  0.0981  0.0317\n",
      "  0.3138  0.1044  0.2963\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.2784  0.0924 -0.1136\n",
      " -0.0158 -0.2920 -0.2749\n",
      "  0.2387  0.2348 -0.0907\n",
      "\n",
      "(10,0 ,.,.) = \n",
      " -0.2507 -0.2515  0.0426\n",
      "  0.3320 -0.2937  0.1951\n",
      "  0.2494 -0.0089  0.3184\n",
      "\n",
      "(11,0 ,.,.) = \n",
      "  0.2545 -0.0159 -0.0383\n",
      " -0.0205 -0.0471  0.1898\n",
      " -0.0711 -0.1697  0.1344\n",
      "\n",
      "(12,0 ,.,.) = \n",
      " -0.2029  0.0128 -0.2283\n",
      "  0.1445  0.0318 -0.3202\n",
      "  0.0835 -0.0312  0.1012\n",
      "\n",
      "(13,0 ,.,.) = \n",
      "  0.0883 -0.0241  0.1354\n",
      " -0.0265 -0.1899 -0.0248\n",
      " -0.0867  0.1241  0.2544\n",
      "\n",
      "(14,0 ,.,.) = \n",
      "  0.2303 -0.0550  0.0022\n",
      "  0.2558 -0.0871 -0.3132\n",
      " -0.1500 -0.3045 -0.0486\n",
      "\n",
      "(15,0 ,.,.) = \n",
      " -0.1173 -0.2260 -0.0456\n",
      " -0.0579 -0.1580 -0.3062\n",
      "  0.2317 -0.0549 -0.1359\n",
      "[torch.FloatTensor of size 16x1x3x3]\n",
      "\n",
      "conv1.bias \n",
      " 0.2406\n",
      "-0.1644\n",
      "-0.0694\n",
      " 0.0311\n",
      "-0.1326\n",
      "-0.3078\n",
      "-0.1841\n",
      "-0.2082\n",
      " 0.0149\n",
      " 0.1085\n",
      "-0.2332\n",
      "-0.1657\n",
      " 0.2912\n",
      " 0.2334\n",
      " 0.2623\n",
      "-0.2786\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "conv2.weight \n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  2.3742 -1.5434  5.3004\n",
      "  2.6614  1.2208 -0.5664\n",
      "  3.8649  3.9673  7.8994\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  3.6491 -4.4424 -5.7785\n",
      " -5.8591  4.1156  2.5734\n",
      " -0.0845  4.4970 -1.4722\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  5.3227 -2.8972 -8.3309\n",
      "  5.2748 -6.8069 -5.1469\n",
      "  1.8618  2.1002  0.6436\n",
      "   ...\n",
      "\n",
      "(0 ,13,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.8360  0.5391 -2.6510\n",
      " -3.5973 -0.0310 -5.1149\n",
      " -0.5966  3.0481  7.4268\n",
      "\n",
      "(0 ,14,.,.) = \n",
      "1.00000e-02 *\n",
      " -1.5875  4.7723  1.4272\n",
      " -3.6749 -8.0083  6.5737\n",
      "  1.0600  1.2246  1.9348\n",
      "\n",
      "(0 ,15,.,.) = \n",
      "1.00000e-02 *\n",
      "  7.9822  0.1811 -1.6439\n",
      "  7.0849 -0.4524 -0.2752\n",
      " -3.7207  6.1476  5.9373\n",
      "     ⋮ \n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -3.6156  1.0916 -1.2542\n",
      " -8.3131 -5.8097  7.2918\n",
      "  0.2288 -3.0768  2.4230\n",
      "\n",
      "(1 ,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  4.8460 -1.6410  1.9379\n",
      "  7.0205 -1.4685 -5.7264\n",
      "  7.4702 -7.9677 -4.4491\n",
      "\n",
      "(1 ,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  3.9573 -4.7772  3.1215\n",
      "  6.4280 -2.2882  6.2599\n",
      "  6.4362 -2.7337 -8.2732\n",
      "   ...\n",
      "\n",
      "(1 ,13,.,.) = \n",
      "1.00000e-02 *\n",
      " -3.4305  0.8174  0.9912\n",
      "  2.3570  2.1222  6.6839\n",
      "  8.0940 -0.6621 -7.5512\n",
      "\n",
      "(1 ,14,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.2949  6.3238 -5.4236\n",
      "  1.7037  6.8006  6.0744\n",
      "  8.0176 -0.3050  7.8271\n",
      "\n",
      "(1 ,15,.,.) = \n",
      "1.00000e-02 *\n",
      "  3.7458 -4.2393  4.8625\n",
      " -1.5493 -2.1137 -4.2895\n",
      " -5.9893 -6.6281  0.1522\n",
      "     ⋮ \n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.9514 -6.6494  4.0927\n",
      " -3.0251  3.3090  6.4558\n",
      " -0.1533  6.3896  7.5236\n",
      "\n",
      "(2 ,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  5.3114 -2.1353  1.3245\n",
      "  1.3503  7.6222 -3.4260\n",
      "  4.3247 -5.5157 -5.9097\n",
      "\n",
      "(2 ,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  5.4051  3.0926  3.7428\n",
      "  7.1416  0.4397 -0.7915\n",
      "  1.0816  4.0204 -3.6990\n",
      "   ...\n",
      "\n",
      "(2 ,13,.,.) = \n",
      "1.00000e-02 *\n",
      " -0.2888 -4.4459  4.6867\n",
      " -4.5081  7.3049  8.2276\n",
      " -7.7961 -6.0490 -1.5167\n",
      "\n",
      "(2 ,14,.,.) = \n",
      "1.00000e-02 *\n",
      " -2.7026  3.0859  5.4031\n",
      "  8.0942  7.1606  2.3498\n",
      " -2.5045  2.4764  1.5888\n",
      "\n",
      "(2 ,15,.,.) = \n",
      "1.00000e-02 *\n",
      " -2.2068  4.8851 -8.1422\n",
      "  3.7938 -2.5678 -3.5421\n",
      "  0.7159  7.7507 -1.8836\n",
      "...   \n",
      "     ⋮ \n",
      "\n",
      "(29,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -2.4314  4.3471 -2.5371\n",
      "  2.2185 -7.4015  2.1658\n",
      "  0.1909  4.0826 -2.9370\n",
      "\n",
      "(29,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  4.9382  6.2082 -1.1528\n",
      " -4.6983  5.6447  1.7778\n",
      " -4.4673 -4.6903 -4.6651\n",
      "\n",
      "(29,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.6241 -8.2482  7.0684\n",
      "  7.2358  6.2532 -5.2380\n",
      " -1.3798  3.5408  2.8980\n",
      "   ...\n",
      "\n",
      "(29,13,.,.) = \n",
      "1.00000e-02 *\n",
      "  2.9712  8.2185  7.2880\n",
      "  3.4153  1.4199 -5.8362\n",
      "  3.7000  0.3432 -1.5454\n",
      "\n",
      "(29,14,.,.) = \n",
      "1.00000e-02 *\n",
      "  3.4317 -2.5029  2.6605\n",
      "  1.9927  6.0984  5.0908\n",
      " -3.6674  2.9470  2.9902\n",
      "\n",
      "(29,15,.,.) = \n",
      "1.00000e-02 *\n",
      "  4.9734 -2.7758 -4.0540\n",
      " -1.9408 -3.1293 -4.2459\n",
      "  2.2840  2.9195  1.8423\n",
      "     ⋮ \n",
      "\n",
      "(30,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -5.2416 -5.9575 -1.6001\n",
      "  4.4317  8.1244  7.2015\n",
      "  5.7720  2.3167  3.1872\n",
      "\n",
      "(30,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  6.1136  1.0847 -1.7985\n",
      " -7.9241 -5.8002  5.0728\n",
      " -2.0633 -7.9946 -7.1626\n",
      "\n",
      "(30,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -5.2652  4.6961  2.9384\n",
      "  3.0324 -3.9829  2.3097\n",
      " -0.5702  7.4996  6.9906\n",
      "   ...\n",
      "\n",
      "(30,13,.,.) = \n",
      "1.00000e-02 *\n",
      "  2.3753  3.7420 -6.8656\n",
      "  0.4351  7.4806 -3.6069\n",
      " -6.3235 -5.6667  1.9278\n",
      "\n",
      "(30,14,.,.) = \n",
      "1.00000e-02 *\n",
      "  2.1595  5.5101  6.8499\n",
      " -7.8980 -4.0796  4.9364\n",
      " -2.2274 -1.3134  6.5583\n",
      "\n",
      "(30,15,.,.) = \n",
      "1.00000e-02 *\n",
      "  5.9992  5.7625 -1.4297\n",
      "  3.6351 -4.6393 -2.4058\n",
      "  5.6943 -7.9623  1.6271\n",
      "     ⋮ \n",
      "\n",
      "(31,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      " -3.6416 -8.0015  4.0937\n",
      "  2.8446  7.2976  2.3704\n",
      "  7.5920  3.4742  1.5203\n",
      "\n",
      "(31,1 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  5.4007  7.2184 -8.1465\n",
      " -4.0274  3.5626  4.0503\n",
      "  3.7586  7.5981 -3.9167\n",
      "\n",
      "(31,2 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  4.4936 -8.1587 -7.2772\n",
      "  7.7259  5.1622 -4.1659\n",
      " -3.0962 -7.1181 -5.9483\n",
      "   ...\n",
      "\n",
      "(31,13,.,.) = \n",
      "1.00000e-02 *\n",
      " -4.0184  0.1083 -1.3549\n",
      " -8.0506 -4.3248 -7.6131\n",
      " -2.4335  7.0008  0.0987\n",
      "\n",
      "(31,14,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.0222 -2.3766  2.8245\n",
      "  0.2553 -1.5885  7.1742\n",
      "  5.8907  0.5794 -0.6132\n",
      "\n",
      "(31,15,.,.) = \n",
      "1.00000e-02 *\n",
      " -1.1836 -6.6444  2.9309\n",
      "  1.5570 -6.6881  1.1862\n",
      " -4.0438 -3.7288 -2.7102\n",
      "[torch.FloatTensor of size 32x16x3x3]\n",
      "\n",
      "conv2.bias \n",
      "1.00000e-02 *\n",
      "  1.8073\n",
      " -7.1349\n",
      "  2.8686\n",
      "  4.9812\n",
      "  1.2670\n",
      " -0.4851\n",
      " -5.7671\n",
      "  2.8575\n",
      " -1.7303\n",
      "  0.9646\n",
      " -0.2854\n",
      " -4.5865\n",
      "  2.5437\n",
      " -2.8748\n",
      "  0.8164\n",
      " -5.1161\n",
      " -8.0042\n",
      "  2.0408\n",
      " -3.9130\n",
      "  7.7404\n",
      " -7.8681\n",
      " -6.4190\n",
      "  4.8454\n",
      " -3.2823\n",
      " -0.3654\n",
      " -1.3067\n",
      " -5.2558\n",
      "  1.3907\n",
      " -2.1398\n",
      " -5.0424\n",
      " -7.0280\n",
      "  7.7318\n",
      "[torch.FloatTensor of size 32]\n",
      "\n",
      "fc1.weight \n",
      " 1.2885e-03 -1.5316e-04  1.5031e-03  ...  -1.2686e-03 -1.3162e-03 -1.7160e-03\n",
      "-1.2427e-03  2.1592e-03 -1.8716e-03  ...  -2.1198e-03 -1.8154e-03 -1.3650e-03\n",
      " 1.4005e-03  1.7374e-03 -1.4698e-03  ...   6.7019e-04 -1.9496e-03 -1.9820e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-1.4479e-03  1.7629e-03  1.4661e-04  ...  -1.5673e-03 -4.9010e-04 -1.8136e-04\n",
      "-9.6004e-04 -1.1788e-03  1.4456e-03  ...   1.5988e-03 -1.6745e-03 -6.9512e-04\n",
      "-1.1600e-03  6.1075e-04  2.0213e-03  ...   8.5536e-04 -9.4785e-04 -1.3421e-03\n",
      "[torch.FloatTensor of size 64x184832]\n",
      "\n",
      "fc1.bias \n",
      "1.00000e-03 *\n",
      " -0.8205\n",
      " -0.9263\n",
      " -0.3538\n",
      "  1.0636\n",
      " -1.7284\n",
      "  0.7301\n",
      "  1.6668\n",
      "  1.2499\n",
      "  0.2151\n",
      "  2.0748\n",
      "  2.0058\n",
      "  0.9348\n",
      "  2.0270\n",
      "  0.1957\n",
      "  1.0911\n",
      " -1.7709\n",
      "  1.0719\n",
      " -1.2222\n",
      " -0.0201\n",
      "  1.2138\n",
      " -1.4902\n",
      " -1.6712\n",
      "  2.0101\n",
      " -0.4546\n",
      " -1.3401\n",
      "  2.1022\n",
      " -0.9866\n",
      "  1.5479\n",
      " -1.4917\n",
      " -1.0468\n",
      "  0.1764\n",
      "  0.4175\n",
      " -2.2615\n",
      "  0.1086\n",
      "  2.0840\n",
      "  2.0049\n",
      "  0.1020\n",
      " -1.2339\n",
      "  0.9941\n",
      "  0.1768\n",
      " -2.1577\n",
      " -1.3764\n",
      " -0.9801\n",
      "  1.3459\n",
      "  1.4358\n",
      "  2.1974\n",
      "  2.2913\n",
      "  0.9225\n",
      " -0.4392\n",
      "  2.2732\n",
      " -1.9169\n",
      " -0.0183\n",
      "  1.9901\n",
      " -1.3925\n",
      " -0.0306\n",
      "  2.0462\n",
      "  2.2132\n",
      "  2.0107\n",
      " -0.7278\n",
      "  1.5693\n",
      " -1.9651\n",
      " -0.4441\n",
      " -2.2178\n",
      " -0.4159\n",
      "[torch.FloatTensor of size 64]\n",
      "\n",
      "fc2.weight \n",
      " 0.0082  0.1000  0.0223  ...   0.1102  0.0231 -0.1004\n",
      "-0.0647 -0.0388  0.1134  ...  -0.0052  0.0152  0.0346\n",
      "-0.0726  0.0621 -0.0366  ...  -0.0509  0.0897  0.0025\n",
      "          ...             ⋱             ...          \n",
      " 0.0767 -0.0632 -0.1122  ...   0.0362  0.0613 -0.0951\n",
      " 0.0594  0.0315 -0.1008  ...  -0.1192 -0.0932  0.0533\n",
      " 0.1083  0.0126 -0.0971  ...   0.0296  0.0369  0.0176\n",
      "[torch.FloatTensor of size 32x64]\n",
      "\n",
      "fc2.bias \n",
      "-0.0040\n",
      "-0.0947\n",
      "-0.0864\n",
      " 0.1205\n",
      "-0.0594\n",
      "-0.1198\n",
      " 0.1200\n",
      "-0.0887\n",
      "-0.0118\n",
      " 0.1022\n",
      "-0.0277\n",
      " 0.0829\n",
      " 0.0627\n",
      "-0.0121\n",
      " 0.0064\n",
      " 0.0852\n",
      " 0.0714\n",
      " 0.0217\n",
      " 0.0992\n",
      "-0.1057\n",
      "-0.0530\n",
      " 0.0025\n",
      "-0.0434\n",
      "-0.0639\n",
      " 0.0548\n",
      "-0.0082\n",
      "-0.0483\n",
      "-0.0708\n",
      " 0.0054\n",
      " 0.0271\n",
      " 0.0607\n",
      "-0.0291\n",
      "[torch.FloatTensor of size 32]\n",
      "\n",
      "fc3.weight \n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0107 -0.0791  0.0262 -0.0664  0.1758 -0.0346 -0.0901 -0.0149 -0.0591  0.0451\n",
      " 0.0908  0.0555 -0.1466  0.0009  0.0149  0.0950  0.0514  0.0982 -0.1128  0.1312\n",
      "-0.1615  0.1157 -0.1577 -0.1181 -0.1219  0.1052  0.0337  0.0094 -0.0544  0.0529\n",
      "\n",
      "Columns 10 to 19 \n",
      "-0.1352  0.1762 -0.1586  0.0342  0.0427 -0.1717 -0.0051 -0.0973 -0.1475  0.1121\n",
      " 0.1362 -0.1061  0.0747  0.0564 -0.0683 -0.0009 -0.0088 -0.0001  0.0918  0.1586\n",
      " 0.0369  0.0333  0.0684 -0.0219  0.0849  0.1354  0.0805 -0.0519  0.1406  0.1380\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0479  0.1516  0.1371  0.0132 -0.1570 -0.0699 -0.1575  0.0860 -0.0640 -0.0034\n",
      "-0.0724  0.1099  0.0429 -0.0172 -0.0809 -0.1059 -0.0550  0.1269 -0.0240  0.0518\n",
      " 0.0189 -0.0973 -0.0405 -0.1213 -0.0795  0.1197 -0.0298 -0.1494 -0.0395 -0.0836\n",
      "\n",
      "Columns 30 to 31 \n",
      "-0.0955 -0.1388\n",
      "-0.1311 -0.0670\n",
      " 0.0109  0.0096\n",
      "[torch.FloatTensor of size 3x32]\n",
      "\n",
      "fc3.bias \n",
      "-0.1204\n",
      "-0.1674\n",
      " 0.0386\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "fc4.weight \n",
      "-3.3537e-03  8.1920e-03  1.8633e-03  ...  -1.2490e-02  6.2005e-03 -1.6778e-03\n",
      " 2.6218e-03  5.3471e-03  8.7051e-03  ...  -3.5225e-03 -1.2083e-02  6.7977e-03\n",
      " 7.9131e-03 -6.4065e-03 -3.6313e-04  ...  -8.9396e-03  9.7598e-04 -1.0206e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 6.4462e-04 -9.4351e-03 -8.4462e-03  ...   3.3906e-04 -2.3690e-03  5.5520e-03\n",
      " 5.4554e-03  7.5689e-03  4.8601e-03  ...  -2.1327e-03  1.0819e-02 -8.5227e-03\n",
      "-8.5074e-03 -1.4384e-03 -1.1526e-02  ...   1.2233e-02 -7.9069e-03 -4.7936e-03\n",
      "[torch.FloatTensor of size 256x6400]\n",
      "\n",
      "fc4.bias \n",
      "1.00000e-02 *\n",
      "  0.2157\n",
      " -0.6634\n",
      "  0.4288\n",
      "  0.4678\n",
      "  1.0020\n",
      "  0.4930\n",
      "  0.7904\n",
      " -1.2218\n",
      " -0.8609\n",
      "  1.2079\n",
      "  0.2120\n",
      " -1.1945\n",
      " -0.5387\n",
      " -0.8502\n",
      " -1.0103\n",
      " -0.9168\n",
      "  0.5939\n",
      " -0.2718\n",
      "  0.4279\n",
      "  0.3046\n",
      " -0.5926\n",
      "  1.0075\n",
      "  0.7677\n",
      "  0.4389\n",
      "  0.2593\n",
      "  0.3995\n",
      "  0.0003\n",
      "  0.4359\n",
      "  1.1117\n",
      " -0.0103\n",
      " -0.1272\n",
      "  1.1172\n",
      "  0.0516\n",
      " -0.0971\n",
      " -1.1715\n",
      " -0.0392\n",
      " -0.9896\n",
      " -0.5764\n",
      " -0.0086\n",
      "  1.0193\n",
      " -0.5894\n",
      " -0.8409\n",
      "  0.8665\n",
      " -0.7089\n",
      "  1.1708\n",
      " -0.5013\n",
      "  0.6316\n",
      " -1.1077\n",
      "  0.4264\n",
      "  0.9552\n",
      " -1.0853\n",
      "  0.3061\n",
      "  0.7194\n",
      " -0.4074\n",
      " -0.7643\n",
      "  0.4619\n",
      "  0.4770\n",
      " -0.7898\n",
      " -1.0917\n",
      "  0.2293\n",
      " -1.1678\n",
      "  0.4682\n",
      "  0.9808\n",
      "  0.6671\n",
      " -1.1242\n",
      "  0.6862\n",
      "  0.7477\n",
      "  0.8512\n",
      " -0.0741\n",
      " -0.0137\n",
      " -0.1904\n",
      "  0.2329\n",
      " -0.0571\n",
      " -0.0775\n",
      "  0.7997\n",
      " -1.1233\n",
      "  0.8046\n",
      "  0.4471\n",
      "  0.0344\n",
      " -1.0912\n",
      "  0.3104\n",
      "  0.1387\n",
      " -0.2863\n",
      "  1.1940\n",
      "  0.3166\n",
      " -0.0798\n",
      "  0.9856\n",
      "  0.1163\n",
      " -0.9339\n",
      "  0.3741\n",
      "  0.4844\n",
      "  1.1988\n",
      "  0.8522\n",
      " -0.2196\n",
      "  0.1487\n",
      " -1.0127\n",
      " -0.6201\n",
      "  1.1665\n",
      "  0.0626\n",
      " -0.5499\n",
      "  0.6165\n",
      " -0.4413\n",
      "  0.1322\n",
      "  0.6462\n",
      "  0.4966\n",
      " -0.9577\n",
      "  0.6675\n",
      "  0.1397\n",
      "  0.7944\n",
      " -0.8906\n",
      "  0.7152\n",
      "  0.3092\n",
      "  0.9989\n",
      "  0.2925\n",
      " -0.3119\n",
      " -0.3126\n",
      "  0.1284\n",
      "  0.9912\n",
      "  0.5944\n",
      "  0.2215\n",
      "  1.1305\n",
      " -0.9994\n",
      " -0.5846\n",
      "  0.4987\n",
      "  1.1237\n",
      " -0.5577\n",
      "  0.1379\n",
      "  0.0949\n",
      "  0.4584\n",
      "  0.4017\n",
      "  1.1015\n",
      "  0.5078\n",
      " -0.4566\n",
      "  0.2547\n",
      " -0.6690\n",
      "  0.8235\n",
      " -0.7009\n",
      "  1.1106\n",
      " -1.0261\n",
      "  1.1796\n",
      " -0.0919\n",
      " -0.7663\n",
      " -0.8155\n",
      " -0.4466\n",
      " -1.0430\n",
      " -0.4836\n",
      " -0.3459\n",
      "  1.1851\n",
      "  0.2675\n",
      "  1.2383\n",
      " -0.0855\n",
      "  0.0673\n",
      " -1.2202\n",
      "  1.1188\n",
      "  1.0121\n",
      " -0.2570\n",
      "  0.0471\n",
      "  0.0258\n",
      " -0.5496\n",
      " -0.5877\n",
      " -0.6651\n",
      "  0.9864\n",
      "  1.1043\n",
      " -0.1171\n",
      "  0.5177\n",
      "  0.4998\n",
      " -0.1054\n",
      "  0.8713\n",
      " -0.4571\n",
      " -0.1116\n",
      " -1.2240\n",
      "  0.4353\n",
      " -0.5307\n",
      " -1.0592\n",
      " -0.8486\n",
      "  0.2451\n",
      "  0.1634\n",
      " -1.1445\n",
      " -1.0560\n",
      " -0.6770\n",
      "  0.9134\n",
      "  0.2249\n",
      "  0.1845\n",
      "  0.6225\n",
      " -1.1674\n",
      "  0.5725\n",
      "  0.8152\n",
      " -1.0491\n",
      "  1.2324\n",
      " -0.3713\n",
      " -0.5028\n",
      "  0.1232\n",
      " -0.0437\n",
      "  0.8150\n",
      "  0.8475\n",
      " -0.4914\n",
      " -0.6366\n",
      " -0.7249\n",
      " -0.6461\n",
      " -1.2275\n",
      " -0.4378\n",
      " -0.5467\n",
      "  0.3326\n",
      " -0.2053\n",
      "  0.0359\n",
      "  1.0385\n",
      " -0.9509\n",
      " -0.9541\n",
      "  0.6420\n",
      "  0.6352\n",
      " -1.1735\n",
      " -0.4142\n",
      "  0.9745\n",
      "  0.6983\n",
      "  0.6965\n",
      " -0.7906\n",
      " -1.1097\n",
      "  0.4108\n",
      " -0.1444\n",
      " -0.4881\n",
      " -0.3682\n",
      "  0.0178\n",
      " -0.3699\n",
      "  1.1253\n",
      " -0.6143\n",
      "  0.5882\n",
      " -0.0169\n",
      " -0.8419\n",
      "  0.8037\n",
      "  0.9548\n",
      "  0.6844\n",
      " -0.5898\n",
      " -1.1767\n",
      " -0.6405\n",
      "  0.0563\n",
      " -0.7221\n",
      " -0.5936\n",
      " -0.5037\n",
      " -0.1115\n",
      "  0.8244\n",
      " -0.9740\n",
      " -0.6584\n",
      " -0.9804\n",
      "  1.2217\n",
      "  0.7265\n",
      "  0.6117\n",
      " -0.1045\n",
      "  0.5355\n",
      "  0.6650\n",
      " -1.0508\n",
      "  0.0569\n",
      "  0.9817\n",
      " -0.7847\n",
      "  1.1410\n",
      "  0.3126\n",
      "  0.3375\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "fc5.weight \n",
      " 1.5626e-02 -3.3280e-02 -4.4259e-02  ...   5.2207e-03 -1.7405e-02 -4.6953e-02\n",
      " 5.6793e-02  1.9974e-02 -1.6909e-02  ...  -6.9352e-03  1.4826e-03  4.1765e-02\n",
      "-5.4093e-02 -4.1460e-02 -4.2731e-02  ...   1.3032e-03  4.6881e-02  5.6302e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-2.8026e-02  1.0652e-02 -4.7879e-02  ...   2.8092e-02 -3.9056e-05  9.9829e-03\n",
      " 5.6116e-02 -6.8545e-05  5.3942e-02  ...  -4.0978e-02  2.6214e-02  3.6166e-02\n",
      "-1.7723e-03  1.8751e-02 -2.6808e-02  ...   5.6718e-02 -1.9370e-03  1.7420e-02\n",
      "[torch.FloatTensor of size 256x256]\n",
      "\n",
      "fc5.bias \n",
      "1.00000e-02 *\n",
      "  5.3921\n",
      " -0.9054\n",
      " -0.7399\n",
      " -2.8221\n",
      "  3.6017\n",
      " -0.5726\n",
      " -3.3585\n",
      "  2.6010\n",
      " -4.8226\n",
      " -1.2026\n",
      "  4.0178\n",
      "  2.1482\n",
      "  4.5570\n",
      "  4.0661\n",
      " -5.3422\n",
      " -4.3822\n",
      " -1.6628\n",
      "  0.1307\n",
      "  0.6150\n",
      "  5.4064\n",
      " -3.8577\n",
      "  1.8973\n",
      "  3.9318\n",
      "  5.0643\n",
      "  3.5431\n",
      " -2.0591\n",
      "  1.3522\n",
      "  5.9170\n",
      " -2.5833\n",
      " -0.3869\n",
      " -5.1572\n",
      " -1.2735\n",
      " -5.9689\n",
      "  3.2154\n",
      " -4.0985\n",
      "  3.9749\n",
      " -5.2647\n",
      " -0.1497\n",
      "  3.0775\n",
      " -1.8563\n",
      " -3.9904\n",
      " -3.9490\n",
      "  0.2728\n",
      "  1.3421\n",
      " -4.6567\n",
      " -3.6187\n",
      " -0.7880\n",
      "  4.5475\n",
      "  4.1180\n",
      " -5.9639\n",
      "  3.6743\n",
      " -3.5209\n",
      "  0.6560\n",
      " -5.0147\n",
      " -0.8095\n",
      " -0.3140\n",
      " -1.1826\n",
      " -1.0382\n",
      " -5.5959\n",
      " -3.5189\n",
      "  4.0809\n",
      "  2.2047\n",
      " -5.4639\n",
      " -3.6472\n",
      "  0.5525\n",
      "  1.2343\n",
      "  3.4264\n",
      " -4.2636\n",
      "  5.2115\n",
      "  1.3005\n",
      "  2.5482\n",
      " -1.1026\n",
      " -3.4109\n",
      "  5.6205\n",
      "  6.0082\n",
      " -1.6851\n",
      "  2.8701\n",
      "  0.1114\n",
      "  5.3660\n",
      " -1.4479\n",
      " -1.2215\n",
      " -0.6378\n",
      " -1.3804\n",
      "  4.3564\n",
      "  2.1940\n",
      " -4.7244\n",
      "  1.6706\n",
      "  4.6977\n",
      " -2.8299\n",
      " -4.2381\n",
      "  0.3601\n",
      " -0.3421\n",
      " -3.1443\n",
      " -3.4043\n",
      "  2.3851\n",
      " -3.9678\n",
      "  1.8766\n",
      " -5.3756\n",
      "  1.1626\n",
      "  2.5368\n",
      " -4.4036\n",
      " -5.5193\n",
      "  4.1543\n",
      "  5.4639\n",
      " -0.9277\n",
      " -4.9504\n",
      " -4.9529\n",
      " -1.0976\n",
      "  5.0260\n",
      " -2.4363\n",
      " -5.4948\n",
      "  5.9999\n",
      " -4.0408\n",
      "  2.3925\n",
      "  5.9636\n",
      "  0.9659\n",
      "  0.5731\n",
      "  2.4873\n",
      "  3.4132\n",
      " -1.6667\n",
      "  5.2242\n",
      " -1.8160\n",
      "  0.0688\n",
      "  2.4819\n",
      " -5.3354\n",
      "  0.8771\n",
      " -2.9652\n",
      " -1.2245\n",
      " -5.9854\n",
      "  0.5927\n",
      "  1.6590\n",
      "  5.8794\n",
      "  2.8766\n",
      "  1.0465\n",
      "  2.8365\n",
      " -5.4572\n",
      "  4.3388\n",
      "  1.8333\n",
      "  2.9291\n",
      "  6.0455\n",
      " -5.1047\n",
      " -3.4193\n",
      "  1.8785\n",
      "  4.8789\n",
      " -2.1837\n",
      "  2.8350\n",
      "  3.1168\n",
      " -0.8981\n",
      " -5.4424\n",
      " -3.5101\n",
      "  1.0326\n",
      "  4.9769\n",
      " -2.8125\n",
      " -0.2678\n",
      " -3.7843\n",
      " -5.1651\n",
      " -0.0187\n",
      " -6.1322\n",
      "  5.7119\n",
      " -1.9416\n",
      " -1.9368\n",
      " -0.6726\n",
      "  1.4928\n",
      "  2.7951\n",
      " -4.7018\n",
      " -0.1168\n",
      "  6.1785\n",
      "  6.0176\n",
      " -6.2012\n",
      " -0.1856\n",
      " -1.7287\n",
      " -4.9576\n",
      "  0.4354\n",
      " -2.3088\n",
      "  3.1780\n",
      "  3.1165\n",
      "  6.0706\n",
      "  6.2012\n",
      " -3.3178\n",
      " -0.4393\n",
      "  4.1173\n",
      "  2.7777\n",
      " -3.9534\n",
      "  0.5642\n",
      "  4.7915\n",
      " -0.4043\n",
      "  0.7679\n",
      " -5.8827\n",
      "  6.2096\n",
      "  2.8614\n",
      "  1.6495\n",
      " -0.1787\n",
      " -0.5520\n",
      " -2.7416\n",
      "  1.7924\n",
      "  0.1980\n",
      " -0.5140\n",
      " -4.0123\n",
      " -3.6668\n",
      " -0.7592\n",
      "  1.4885\n",
      "  0.8407\n",
      "  1.9880\n",
      " -4.2016\n",
      " -1.2558\n",
      " -3.8260\n",
      "  1.1854\n",
      "  6.0388\n",
      "  0.7926\n",
      " -4.9093\n",
      "  1.9710\n",
      "  0.4822\n",
      "  3.6146\n",
      " -3.0350\n",
      " -1.5807\n",
      "  3.4729\n",
      "  6.2475\n",
      "  4.7181\n",
      " -2.0455\n",
      "  4.2012\n",
      " -3.6630\n",
      "  0.0397\n",
      "  2.3443\n",
      " -2.4248\n",
      "  3.3135\n",
      "  4.6184\n",
      " -4.7475\n",
      "  4.8564\n",
      "  4.9918\n",
      "  4.6496\n",
      " -5.7964\n",
      " -2.9949\n",
      "  2.0075\n",
      "  2.8419\n",
      "  5.3453\n",
      "  0.4770\n",
      " -0.8321\n",
      "  1.2515\n",
      " -3.7070\n",
      "  5.5203\n",
      "  6.2496\n",
      "  4.7361\n",
      " -4.4215\n",
      " -0.5392\n",
      " -0.0396\n",
      "  0.0067\n",
      "  1.7911\n",
      " -3.6793\n",
      "  0.6971\n",
      " -4.9435\n",
      "  0.5673\n",
      " -0.4427\n",
      "  6.0224\n",
      " -5.6148\n",
      " -4.0754\n",
      "  3.6039\n",
      "[torch.FloatTensor of size 256]\n",
      "\n",
      "fc6.weight \n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      "  5.7831 -3.9738  3.8141  1.4441 -2.0368  2.5490  3.0651 -1.9657  5.8028  4.9143\n",
      "  2.3525  2.5525 -2.8405 -6.2369  5.3887  0.0872 -0.0318  0.7269 -0.6820  3.1044\n",
      " -0.4301 -1.4311 -0.2249  5.3191 -5.6834 -4.5088  4.3379  5.4282 -1.7431 -5.8030\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      " -1.0247 -0.7320 -1.9917  1.3033 -4.4923 -0.5947 -5.5467  2.9294  2.9762 -5.9982\n",
      " -4.6226  5.3206  1.6093 -4.6126 -3.5691  0.6598 -4.9949  6.1906  2.1937 -0.3009\n",
      " -6.0588 -0.7595  3.2811 -5.3920 -2.2611 -0.5743  5.5445 -0.8709 -3.2167  5.4477\n",
      "\n",
      "Columns 20 to 29 \n",
      "1.00000e-02 *\n",
      " -5.3454 -4.1420  4.4902 -0.1208  3.9255 -5.6241 -5.0799  5.4903  0.6998 -0.3930\n",
      " -5.7580  5.7235  5.1210  4.0103 -2.8363 -3.3625  2.7524  1.8443  4.0727  0.7379\n",
      "  5.7205 -0.5711 -4.0218  3.7869  0.1871 -0.1214 -6.1645 -4.3775  4.6421  3.1923\n",
      "\n",
      "Columns 30 to 39 \n",
      "1.00000e-02 *\n",
      " -5.7308 -1.5811  4.2455 -1.1087 -0.8839  0.0433 -4.8604  3.3389 -1.2144 -1.5718\n",
      " -1.3060  4.1353 -3.5054 -4.2182  5.2683  3.8585 -4.4880  3.3544  4.8624 -2.8177\n",
      " -3.3535  3.2731 -3.7392  2.2979  3.3549  5.0896 -5.8562 -0.0229  4.2350  3.3359\n",
      "\n",
      "Columns 40 to 49 \n",
      "1.00000e-02 *\n",
      " -1.6431  0.7711  3.0750  2.5433  2.0846  2.3822  3.0275  0.8823  0.9853 -2.5595\n",
      " -6.0992 -1.9050 -2.0644 -1.4431 -0.1985 -1.3483 -3.9218 -3.5961 -4.0887 -4.7107\n",
      " -2.7272  3.4846  4.9674 -5.3097  0.4440 -3.3618 -6.0725  4.0145 -3.1732 -4.5317\n",
      "\n",
      "Columns 50 to 59 \n",
      "1.00000e-02 *\n",
      "  4.4319  5.4315  0.2873 -3.2949 -5.1675 -0.5965  4.7581  3.5584  5.1004 -4.4617\n",
      "  2.1339 -1.9233 -1.7221  4.2782 -4.6299  0.3213  1.9012  1.3519 -3.4836  6.1564\n",
      " -3.5441  5.9296 -0.8935 -3.6615  2.0327  1.9264 -0.2832 -5.3398  3.6185  0.8870\n",
      "\n",
      "Columns 60 to 69 \n",
      "1.00000e-02 *\n",
      " -4.7081 -5.5745 -2.1210  4.4766  0.5751 -2.2714  4.4471  6.0518 -4.8883  3.4282\n",
      "  0.2987  0.9399 -4.8644  5.6739 -3.1263 -5.8809  3.1849  0.7147 -2.2627  0.7621\n",
      "  2.8034  3.9872 -5.8297  5.5601  1.3480 -2.5467  0.3398 -2.6818  1.7035  2.2024\n",
      "\n",
      "Columns 70 to 79 \n",
      "1.00000e-02 *\n",
      " -3.6066  2.8265  4.9776 -5.2038  1.2624  0.3914 -0.7838  0.3095  6.1312 -4.1381\n",
      " -2.1444 -0.0084 -2.7194  6.0013  5.6706  5.6892 -1.9635 -5.6051 -5.4458  5.5376\n",
      "  5.8314  3.9717  0.7646  4.3236  0.2228 -2.3457 -4.7013  0.5730  4.6619 -1.7341\n",
      "\n",
      "Columns 80 to 89 \n",
      "1.00000e-02 *\n",
      " -1.1667 -0.0358 -1.6035  5.8039  0.0454 -0.1309  4.1506 -1.1450  3.1697  4.8345\n",
      "  0.6551 -4.1976 -0.4285 -3.4057 -6.1289 -1.2600 -4.3645 -4.6226 -1.9125 -1.7801\n",
      " -0.7360 -3.1998  1.3838 -4.5241  1.4131 -0.8969  5.8204  5.1885  4.2999 -4.4696\n",
      "\n",
      "Columns 90 to 99 \n",
      "1.00000e-02 *\n",
      "  2.6397  2.7044  3.4988 -2.2783 -1.3625  3.4918 -4.7898 -2.6130 -4.0863  6.0178\n",
      " -1.9002 -0.9769 -2.5607 -4.7604 -2.6795 -4.4225 -2.2197 -2.3229  0.6855  2.7845\n",
      " -4.1768  3.1038 -1.3608  3.5579  4.0956  5.1672 -2.0099  3.1815 -4.3485 -4.6289\n",
      "\n",
      "Columns 100 to 109 \n",
      "1.00000e-02 *\n",
      "  5.1856 -2.0047 -2.8924 -1.8977 -2.4203  3.0900 -0.2079 -3.9460  1.9798  3.3349\n",
      " -5.6819 -4.2327  3.3117  0.3131 -3.5053  5.6154 -4.8879  5.3517 -5.6489  2.4650\n",
      " -3.2485 -0.0428 -5.4875 -1.4333 -0.4832  0.0040 -4.0944 -1.7103 -3.1938  4.6404\n",
      "\n",
      "Columns 110 to 119 \n",
      "1.00000e-02 *\n",
      "  3.5389  2.1449  3.9119 -3.1443  4.7833  1.0222  3.6153  1.0447  0.9319  2.3170\n",
      "  0.6543  4.2823 -2.1172 -3.6606 -0.4038  4.0436  5.0405  5.9777  3.9088 -5.5427\n",
      " -2.1335  2.4337  5.2544 -4.4957  1.2697 -1.2332  0.3075  1.5479 -0.1487 -2.5065\n",
      "\n",
      "Columns 120 to 129 \n",
      "1.00000e-02 *\n",
      " -3.1454  1.5077 -3.9876 -6.1471  5.8313  2.9911  1.8832  2.6551  5.6721  2.5308\n",
      "  0.6807  3.7562 -0.3669  1.7360  0.3886  1.2065 -3.5623  0.7790  5.9760 -0.7128\n",
      " -2.2358  2.5317 -5.0857 -2.9849 -3.9937 -0.8977  4.1852  4.5282  6.1094 -0.5993\n",
      "\n",
      "Columns 130 to 139 \n",
      "1.00000e-02 *\n",
      "  4.0758 -1.9074 -2.6583 -4.2191 -0.6473 -1.6705 -0.4604  3.1665  0.0351  4.7490\n",
      "  6.1553 -5.1123  0.2658  3.4097  5.3755  3.2572  2.2782  4.0920 -5.9883 -0.9632\n",
      "  3.5303 -4.0111  4.7968  2.0806  1.0034 -5.7145  5.3760  3.3956  2.8100  0.6169\n",
      "\n",
      "Columns 140 to 149 \n",
      "1.00000e-02 *\n",
      "  0.5475  5.8662 -5.6496  3.2866  3.0058  0.8325 -3.6194 -3.8374 -2.6230  2.7096\n",
      "  1.8976  5.0743 -1.6746  1.9659  3.6010  1.4170  5.7499  5.1912 -0.0522 -5.6298\n",
      " -1.8564 -4.3748  3.3106  4.8072  3.4265 -6.1578  3.8555  0.4148  3.9013 -6.2109\n",
      "\n",
      "Columns 150 to 159 \n",
      "1.00000e-02 *\n",
      "  1.8027 -4.0422 -0.5542  4.9288 -5.8670 -5.1455 -2.1929  0.2097 -3.8223  0.1055\n",
      "  6.2080 -1.1165 -2.4352 -1.3426  1.8883 -0.3018 -4.6350 -5.3112 -1.3567  4.3321\n",
      "  2.6228  1.8389 -2.8644 -5.0541  2.3994 -6.0963 -1.1662  1.6750  0.5873 -6.0787\n",
      "\n",
      "Columns 160 to 169 \n",
      "1.00000e-02 *\n",
      "  2.4335  0.2855  0.9494  6.0422  1.2999  3.5874  1.9706  4.4252 -5.5194  6.1484\n",
      "  0.9591 -5.8894 -0.2641  3.4473  3.7010  0.3620 -4.6291  3.2069 -3.7353 -4.3745\n",
      "  0.8396 -4.3332 -4.3240 -4.0951  6.0014 -0.2031 -3.4243 -2.7380  5.3028  4.7486\n",
      "\n",
      "Columns 170 to 179 \n",
      "1.00000e-02 *\n",
      " -2.5163  3.4415  3.9129 -2.4800 -1.9289 -1.1328  1.2854  5.6463 -5.8731 -0.9396\n",
      "  2.3155 -0.3867  5.2516 -3.3429 -3.7960  4.0346 -2.3957 -0.8854 -4.3427 -0.5060\n",
      "  6.1308  4.5345  2.5039 -4.4915  4.2802  5.0452 -2.6289 -4.0698  4.8907 -1.2003\n",
      "\n",
      "Columns 180 to 189 \n",
      "1.00000e-02 *\n",
      " -5.3246 -0.7461 -0.3977 -1.3668  1.9284 -0.6828 -3.0423 -2.2136 -4.9414  1.3917\n",
      " -1.3641  1.5930  5.7690  3.2135 -4.5600  0.9016 -1.2166  2.4323 -3.3141 -1.7563\n",
      "  4.4357  0.8291  0.4915  4.3305 -1.9654  0.9212 -2.9883  1.3463 -5.2909 -5.8974\n",
      "\n",
      "Columns 190 to 199 \n",
      "1.00000e-02 *\n",
      " -0.8156  6.0702  2.0301  2.1660 -0.4152 -5.4546  5.1474  5.3013  4.6998 -5.3368\n",
      " -5.2213 -1.8881  4.4680 -5.9013  3.8972 -4.1506 -0.9208 -5.4684 -1.0140 -4.9705\n",
      "  5.7482 -3.2547 -3.5719  5.4940 -1.5035 -2.1436 -0.6216  5.1125  2.1296  3.2399\n",
      "\n",
      "Columns 200 to 209 \n",
      "1.00000e-02 *\n",
      "  0.9720  3.3537  0.1097 -3.8738 -3.7031  3.0117 -4.8746 -5.4280 -0.4931 -3.8267\n",
      " -0.3427 -5.0882  4.8649 -2.1582  0.7352  1.7560  0.5469 -1.6749  1.1937  1.8474\n",
      "  3.5051  5.8282  1.3365  3.1814  4.2309 -2.0978  4.3609  0.7276  1.2217  0.0029\n",
      "\n",
      "Columns 210 to 219 \n",
      "1.00000e-02 *\n",
      " -2.7667 -3.2172  0.8707  0.4247  4.1106 -5.5114 -1.5087  5.5595 -0.1017 -2.9991\n",
      "  5.6264 -4.7187  5.8683 -5.1907  4.1251  4.5760 -5.9190  4.2891 -3.7744 -0.7282\n",
      "  2.8606  3.7165  5.8495  2.2326  1.7447 -3.7220  3.9000  4.9929  5.1379 -3.1350\n",
      "\n",
      "Columns 220 to 229 \n",
      "1.00000e-02 *\n",
      " -3.9361  3.6970  1.5293  1.9309 -4.4599  4.2020  1.7722  2.2250  4.6584  6.0972\n",
      "  0.4773  1.9616  2.4710 -4.2248  5.1022  2.4838  3.2592 -4.6801  3.4161 -3.9997\n",
      "  0.1140 -3.8012  3.4398 -3.1528 -3.5706 -0.6047  5.5239  3.4049  0.4744 -0.5894\n",
      "\n",
      "Columns 230 to 239 \n",
      "1.00000e-02 *\n",
      " -2.3463 -3.6417 -5.3155  0.9276  4.2096  2.0394 -5.5692  5.9189 -3.9738  4.2300\n",
      "  1.0641  3.8879 -2.0777 -0.7484 -1.3559  3.7340  2.8685 -0.4630  0.1123  4.1434\n",
      " -5.4008 -0.9493 -0.5033  2.0668 -4.8655  2.8931 -3.4004  1.1429 -0.1680 -0.7959\n",
      "\n",
      "Columns 240 to 249 \n",
      "1.00000e-02 *\n",
      " -5.9978  1.1886 -3.8538 -5.1711 -3.2861 -4.6250 -5.7708 -4.9094 -4.3220  0.8506\n",
      "  4.8766  5.5767 -5.4334  5.5504  4.0135 -2.4896 -2.1911  2.7555  2.5874 -3.3965\n",
      " -3.7180  4.9881 -2.0144 -4.3637  4.1139  5.5138 -0.9898  4.4533 -0.0538  5.0393\n",
      "\n",
      "Columns 250 to 255 \n",
      "1.00000e-02 *\n",
      " -3.7707  2.0959  3.0709 -2.1809 -0.2099 -1.9105\n",
      "  4.7019 -6.1899  0.8426  2.8407  1.1170 -5.6960\n",
      " -0.1709  1.8016 -2.0448 -3.4641  0.3949 -1.4062\n",
      "[torch.FloatTensor of size 3x256]\n",
      "\n",
      "fc6.bias \n",
      "1.00000e-02 *\n",
      "  2.4725\n",
      " -5.6668\n",
      " -6.1937\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in policy.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for param in policy.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4fd5410833e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print('[Time step {}] Finished step, about to backprop'.format(t))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpolicy_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NOTE: only tried this with batch_size=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                                             \u001b[0;31m#if batch_size > 1, not sure if we need to manually average gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "policy = Policy()\n",
    "policy.train()\n",
    "optimizer = torch.optim.Adam(policy.parameters())\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    optimizer.zero_grad()\n",
    "    policy.episode_reward = 0\n",
    "     \n",
    "    for t in range(100):\n",
    "        # self.env.env.render()\n",
    "        policy_output = policy(observation) # (batch_size, 6)\n",
    "        action = int(torch.max(policy_output, dim=1)[1].data) # torch.max returns (max val, argmax)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        policy.episode_reward += reward\n",
    "        # print('[Time step {}] Finished step, about to backprop'.format(t))\n",
    "        policy_output[:,action].backward(retain_graph=False) # NOTE: only tried this with batch_size=1\n",
    "                                            #if batch_size > 1, not sure if we need to manually average gradients\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    if not done:\n",
    "        print('Force terminated episode after running for 100 steps')\n",
    "    print('[After {} seconds] Reward is {}'.format(time.time()-a, policy.episode_reward))\n",
    "    \n",
    "    for param in policy.parameters():\n",
    "        param.grad *= policy.episode_reward\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
    "fc1 = torch.nn.Linear(206 * 156 * 32, 512)\n",
    "fc2 = torch.nn.Linear(512,6)\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-20bff870d3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m210\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "img = Variable(torch.Tensor(a.reshape(1,3,210,160)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 208, 158])\n",
      "torch.Size([1, 32, 206, 156])\n"
     ]
    }
   ],
   "source": [
    "x = conv1(img)\n",
    "print(x.shape)\n",
    "y = conv2(x)\n",
    "print(y.shape)\n",
    "y = y.view(-1, 32*206*156)\n",
    "z = fc1(y)\n",
    "z = fc2(z)\n",
    "output = logsoftmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  8.2107   4.8800  -6.4096  -6.6704  13.4930   6.7540\n",
       "[torch.FloatTensor of size 1x6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = int(torch.max(z, dim=1)[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[:,action].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = torch.nn.Linear(16, 8)\n",
    "fc2 = torch.nn.Linear(8,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([[i for i in range(16)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fc1(x)\n",
    "y = fc2(y)\n",
    "y = torch.nn.functional.softmax(y, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0997  0.9003\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sampler = torch.distributions.Categorical(y[0])\n",
    "log_prob = action_sampler.log_prob(action_sampler.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 15 \n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "[torch.FloatTensor of size 8x16]\n",
      "\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in fc1.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "(log_prob * 2).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0000 -0.0222 -0.0444 -0.0665 -0.0887 -0.1109 -0.1331 -0.1553 -0.1774 -0.1996\n",
      " 0.0000  0.0242  0.0485  0.0727  0.0970  0.1212  0.1454  0.1697  0.1939  0.2181\n",
      " 0.0000 -0.1092 -0.2185 -0.3277 -0.4370 -0.5462 -0.6554 -0.7647 -0.8739 -0.9832\n",
      " 0.0000 -0.1141 -0.2281 -0.3422 -0.4562 -0.5703 -0.6843 -0.7984 -0.9125 -1.0265\n",
      " 0.0000  0.0941  0.1881  0.2822  0.3763  0.4703  0.5644  0.6585  0.7525  0.8466\n",
      " 0.0000 -0.0893 -0.1787 -0.2680 -0.3573 -0.4466 -0.5360 -0.6253 -0.7146 -0.8039\n",
      " 0.0000 -0.1265 -0.2529 -0.3794 -0.5059 -0.6324 -0.7588 -0.8853 -1.0118 -1.1382\n",
      " 0.0000  0.1025  0.2049  0.3074  0.4099  0.5124  0.6148  0.7173  0.8198  0.9223\n",
      "\n",
      "Columns 10 to 15 \n",
      "-0.2218 -0.2440 -0.2662 -0.2883 -0.3105 -0.3327\n",
      " 0.2424  0.2666  0.2909  0.3151  0.3393  0.3636\n",
      "-1.0924 -1.2016 -1.3109 -1.4201 -1.5293 -1.6386\n",
      "-1.1406 -1.2546 -1.3687 -1.4827 -1.5968 -1.7109\n",
      " 0.9407  1.0347  1.1288  1.2229  1.3169  1.4110\n",
      "-0.8933 -0.9826 -1.0719 -1.1612 -1.2506 -1.3399\n",
      "-1.2647 -1.3912 -1.5177 -1.6441 -1.7706 -1.8971\n",
      " 1.0247  1.1272  1.2297  1.3322  1.4346  1.5371\n",
      "[torch.FloatTensor of size 8x16]\n",
      "\n",
      "Variable containing:\n",
      "-0.0222\n",
      " 0.0242\n",
      "-0.1092\n",
      "-0.1141\n",
      " 0.0941\n",
      "-0.0893\n",
      "-0.1265\n",
      " 0.1025\n",
      "[torch.FloatTensor of size 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in fc1.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 15 \n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "    0     0     0\n",
      "[torch.FloatTensor of size 8x16]\n",
      "\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in fc1.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "next expected at least 1 arguments, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-fa1952f122d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: next expected at least 1 arguments, got 0"
     ]
    }
   ],
   "source": [
    "for a in range(5):\n",
    "    next()\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
