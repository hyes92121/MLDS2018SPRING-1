{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, json, pickle\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 1024\n",
    "test_batch_size = 3000\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args('--batch-size {} --test-batch-size {}'.format(batch_size, test_batch_size).split()) # default: taken from sys.argv\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "MNIST_data_path = '../HW1-1/git_ignored/MNIST/MNIST_data'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(MNIST_data_path, train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) # (mean, std)\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(MNIST_data_path, train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) \n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train_eval_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(MNIST_data_path, train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)) \n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params: 21840\n"
     ]
    }
   ],
   "source": [
    "tensorboard_category = 'batch_{}'.format(batch_size)\n",
    "\n",
    "class CNN_2FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_2FC, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d() # p=0.5 by default\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.max_pool1(self.conv1(x))) # (input, kernel_size)\n",
    "        x = F.relu(self.max_pool2(self.conv2_drop(self.conv2(x)),))\n",
    "        x = x.view(-1, 320) # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN_2FC()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('model params:', num_params)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train() # set to training mode\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, requires_grad=True), Variable(target) # storing gradients for input data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        # get gradients of loss wrt. input here\n",
    "        data.grad #64x1x28x28\n",
    "        # IMPORTANT: after calculating gradient of input data, we do not update on it\n",
    "        data.requires_grad = False\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def trained_epoch(epoch):\n",
    "    model.eval()# set to evaluation mode\n",
    "    trained_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in train_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target) # no intermediate states will be saved\n",
    "        output = model(data)\n",
    "        trained_loss += F.nll_loss(output, target, size_average=False).data[0] # want to average loss over entire test set, not just minibatch\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "    \n",
    "    trained_loss /= len(train_loader.dataset)\n",
    "    trained_accuracy = correct / len(train_loader.dataset)\n",
    "    writer.add_scalar('{}/train_loss'.format(tensorboard_category), trained_loss, epoch) # try add_scalars() later\n",
    "    writer.add_scalar('{}/train_accuracy'.format(tensorboard_category), trained_accuracy, epoch)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        trained_loss, correct, len(train_loader.dataset),\n",
    "        100. * trained_accuracy))\n",
    "            \n",
    "def test(epoch):\n",
    "    model.eval()# set to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target) # no intermediate states will be saved\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # want to average loss over entire test set, not just minibatch\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    writer.add_scalar('{}/test_loss'.format(tensorboard_category), test_loss, epoch) # try add_scalars() later\n",
    "    writer.add_scalar('{}/test_accuracy'.format(tensorboard_category), test_accuracy, epoch)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * test_accuracy))\n",
    "    \n",
    "    writer.export_scalars_to_json(\"./batch_{}.json\".format(batch_size))\n",
    "    \n",
    "def model_eval_and_save(save_path):\n",
    "    model.eval()# eval only affects stuff like dropout and batch norm, doesn't remove the ability to backprop\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    sensitivity_test = 0\n",
    "    sensitivity_train = 0\n",
    "    \n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, requires_grad=True), Variable(target) # no intermediate states will be saved\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target) # not caring about loss value here, just need it to compute gradients\n",
    "        loss.backward()\n",
    "        grads_on_input = np.array(data.grad.data).reshape(-1,28,28) # (1000, 28, 28)\n",
    "        batch_of_sensitivities = np.sqrt(np.sum(grads_on_input**2, axis=(1,2))) # (1000,)\n",
    "        sensitivity_test += np.sum(batch_of_sensitivities)\n",
    "    sensitivity_test /= len(datasets.MNIST(MNIST_data_path, train=False))\n",
    "\n",
    "    for idx, (data, target) in enumerate(train_eval_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, requires_grad=True), Variable(target) # no intermediate states will be saved\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target) # not caring about loss value here, just need it to compute gradients\n",
    "        loss.backward()\n",
    "        grads_on_input = np.array(data.grad.data).reshape(-1,28,28) # (1000, 28, 28)\n",
    "        batch_of_sensitivities = np.sqrt(np.sum(grads_on_input**2, axis=(1,2))) # (1000,)\n",
    "        sensitivity_train += np.sum(batch_of_sensitivities)\n",
    "    sensitivity_train /= len(datasets.MNIST(MNIST_data_path, train=True))\n",
    "    \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    return sensitivity_train, sensitivity_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.360069\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.305241\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.285596\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.269358\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.261107\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.240292\n",
      "\n",
      "Train set: Average loss: 2.1836, Accuracy: 21552/60000 (36%)\n",
      "\n",
      "Test set: Average loss: 2.1807, Accuracy: 3676/10000 (37%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.203102\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 2.169038\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 2.093908\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 2.019388\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 1.964313\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.837047\n",
      "\n",
      "Train set: Average loss: 1.5838, Accuracy: 36854/60000 (61%)\n",
      "\n",
      "Test set: Average loss: 1.5672, Accuracy: 6274/10000 (63%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.754678\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 1.720100\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.596864\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 1.512112\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 1.452248\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.327528\n",
      "\n",
      "Train set: Average loss: 0.9060, Accuracy: 47626/60000 (79%)\n",
      "\n",
      "Test set: Average loss: 0.8836, Accuracy: 8042/10000 (80%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.266255\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 1.249395\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 1.170685\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 1.090933\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 1.049398\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.027056\n",
      "\n",
      "Train set: Average loss: 0.5928, Accuracy: 51103/60000 (85%)\n",
      "\n",
      "Test set: Average loss: 0.5733, Accuracy: 8609/10000 (86%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.962461\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.919341\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.925386\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.888533\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.853790\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.883774\n",
      "\n",
      "Train set: Average loss: 0.4719, Accuracy: 52385/60000 (87%)\n",
      "\n",
      "Test set: Average loss: 0.4558, Accuracy: 8775/10000 (88%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.815340\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.836626\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.791733\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.768593\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.773960\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.719366\n",
      "\n",
      "Train set: Average loss: 0.4020, Accuracy: 53338/60000 (89%)\n",
      "\n",
      "Test set: Average loss: 0.3879, Accuracy: 8918/10000 (89%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.694492\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.672461\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.748669\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.739317\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.663145\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.677678\n",
      "\n",
      "Train set: Average loss: 0.3564, Accuracy: 53943/60000 (90%)\n",
      "\n",
      "Test set: Average loss: 0.3424, Accuracy: 9052/10000 (91%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.683859\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.656812\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.637970\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.671043\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.589116\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.629845\n",
      "\n",
      "Train set: Average loss: 0.3265, Accuracy: 54416/60000 (91%)\n",
      "\n",
      "Test set: Average loss: 0.3153, Accuracy: 9112/10000 (91%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.597077\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.617861\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.608341\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.595712\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.585842\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.550437\n",
      "\n",
      "Train set: Average loss: 0.3000, Accuracy: 54844/60000 (91%)\n",
      "\n",
      "Test set: Average loss: 0.2892, Accuracy: 9184/10000 (92%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.629015\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.592084\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.599847\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.520266\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.598963\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.570226\n",
      "\n",
      "Train set: Average loss: 0.2769, Accuracy: 55196/60000 (92%)\n",
      "\n",
      "Test set: Average loss: 0.2672, Accuracy: 9243/10000 (92%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.559044\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.556435\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.567285\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.559203\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.538741\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.582993\n",
      "\n",
      "Train set: Average loss: 0.2601, Accuracy: 55488/60000 (92%)\n",
      "\n",
      "Test set: Average loss: 0.2513, Accuracy: 9274/10000 (93%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.540181\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.567086\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.473591\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.461895\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.480480\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.519210\n",
      "\n",
      "Train set: Average loss: 0.2407, Accuracy: 55731/60000 (93%)\n",
      "\n",
      "Test set: Average loss: 0.2317, Accuracy: 9316/10000 (93%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.546570\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.488186\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.515226\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.509631\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.495418\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.437866\n",
      "\n",
      "Train set: Average loss: 0.2284, Accuracy: 55950/60000 (93%)\n",
      "\n",
      "Test set: Average loss: 0.2201, Accuracy: 9349/10000 (93%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.459744\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.410519\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.471668\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.396095\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.409011\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.477312\n",
      "\n",
      "Train set: Average loss: 0.2160, Accuracy: 56172/60000 (94%)\n",
      "\n",
      "Test set: Average loss: 0.2089, Accuracy: 9378/10000 (94%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.475436\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.456811\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.450064\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.462225\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.437060\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.379307\n",
      "\n",
      "Train set: Average loss: 0.2068, Accuracy: 56292/60000 (94%)\n",
      "\n",
      "Test set: Average loss: 0.1994, Accuracy: 9408/10000 (94%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.449009\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.473921\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.441485\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.449715\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.439137\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.444950\n",
      "\n",
      "Train set: Average loss: 0.1990, Accuracy: 56432/60000 (94%)\n",
      "\n",
      "Test set: Average loss: 0.1926, Accuracy: 9414/10000 (94%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.438431\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.394602\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.388820\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.454195\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.458691\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.417649\n",
      "\n",
      "Train set: Average loss: 0.1892, Accuracy: 56606/60000 (94%)\n",
      "\n",
      "Test set: Average loss: 0.1824, Accuracy: 9446/10000 (94%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.441238\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.399855\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.388013\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.453669\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.404296\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.462181\n",
      "\n",
      "Train set: Average loss: 0.1846, Accuracy: 56690/60000 (94%)\n",
      "\n",
      "Test set: Average loss: 0.1776, Accuracy: 9456/10000 (95%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.412094\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.387463\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.367094\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.431875\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.389617\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.401754\n",
      "\n",
      "Train set: Average loss: 0.1762, Accuracy: 56832/60000 (95%)\n",
      "\n",
      "Test set: Average loss: 0.1712, Accuracy: 9472/10000 (95%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.434036\n",
      "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.365389\n",
      "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.395431\n",
      "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.386352\n",
      "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.390309\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.390553\n",
      "\n",
      "Train set: Average loss: 0.1691, Accuracy: 56908/60000 (95%)\n",
      "\n",
      "Test set: Average loss: 0.1629, Accuracy: 9490/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    trained_epoch(epoch)\n",
    "    test(epoch)\n",
    "writer.close()\n",
    "\n",
    "save_path = './model_batch_{}.pth'.format(batch_size)\n",
    "sensitivity_train, sensitivity_test = model_eval_and_save(save_path)\n",
    "json.dump({'train': sensitivity_train, 'test': sensitivity_test},\n",
    "          open(\"sensitivity_batch_{}.json\".format(batch_size), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load params into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = TheModelClass(*args, **kwargs)\n",
    "the_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use tensorboardX to draw model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to comment out model's cuda() before writing graph\n",
    "with SummaryWriter(comment='MNIST_2FC') as w:\n",
    "    dummy_input = Variable(torch.rand(1000, 1, 28, 28))\n",
    "    w.add_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logs = {}\n",
    "data = json.load(open('CNN_2FC.json'.format(model_id)))\n",
    "model_logs[model_id] = {'loss': np.array(data['train_loss'])[:,2],\n",
    "                       'accuracy': np.array(data['train_accuracy'])[:,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'loss'\n",
    "x = [i+1 for i in range(len(model_logs[2][mode]))]\n",
    "acc_2FC, = plt.plot(x, model_logs[2][mode], label='2FC')\n",
    "acc_3FC, = plt.plot(x, model_logs[3][mode], label='3FC')\n",
    "acc_5FC, = plt.plot(x, model_logs[5][mode], label='5FC')\n",
    "plt.legend(handles=[acc_2FC, acc_3FC, acc_5FC])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel(mode)\n",
    "plt.title('MNIST')\n",
    "plt.savefig('{}.png'.format(mode))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
