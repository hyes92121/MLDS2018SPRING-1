{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 哲賢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/i2w.pickle', 'rb') as handle:\n",
    "\ta = pickle.load(handle)\n",
    "with open('data/w2i.pickle', 'rb') as handle:\n",
    "\tb = pickle.load(handle)\n",
    "for i in range(10):\n",
    "\tprint(a[i])\n",
    "\tprint(b[a[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = numpy.load('data/weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.46295956,  0.60897344, -0.09038982, ..., -0.14422344,\n",
       "        -0.447958  , -0.10773784],\n",
       "       [ 0.40195522,  0.16258185,  0.3913999 , ..., -0.38018417,\n",
       "        -0.29988715,  0.00189703],\n",
       "       ...,\n",
       "       [ 0.11018498, -0.08562936,  0.04963986, ...,  0.05504006,\n",
       "        -0.0210738 , -0.12578586],\n",
       "       [-0.03119463,  0.03227964, -0.00795191, ...,  0.01224131,\n",
       "         0.07899259,  0.03871416],\n",
       "       [ 0.14568943,  0.04103405,  0.12413269, ..., -0.08994981,\n",
       "         0.13046025, -0.08646748]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 克安 playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-6.1635e-01 -8.1987e-01 -8.3256e-01  ...  -4.8794e-01 -4.8985e-01  9.1782e-01\n",
      "-1.5929e+00  2.6971e-01 -1.0009e+00  ...   6.2423e-01 -3.2411e-01 -1.0822e+00\n",
      " 8.9374e-01 -7.2405e-01 -1.5573e+00  ...   1.2894e+00 -2.2827e+00  4.2213e-01\n",
      "                ...                   ⋱                   ...                \n",
      " 1.8184e-01 -6.3451e-01  8.9738e-01  ...   1.0519e+00  1.3843e+00 -6.2459e-01\n",
      " 5.0422e-01  5.3824e-01 -1.3206e+00  ...  -2.7180e-02 -3.1137e-01 -2.0636e-02\n",
      "-1.0055e+00 -3.4340e-01  7.1548e-01  ...   5.6963e-01  7.4671e-01  7.3272e-01\n",
      "[torch.FloatTensor of size 44089x100]\n",
      "\n",
      "True\n",
      "Parameter containing:\n",
      "-2.3829e-01 -7.4225e-02 -3.9740e-02  ...   8.0199e-03  1.3803e-01  2.7434e-01\n",
      " 3.6569e-01  2.5967e-01 -8.3636e-01  ...  -3.8568e-01  3.5039e-01  2.5933e-01\n",
      " 3.9887e-01  2.1847e-01 -1.7002e-01  ...  -2.9260e-01  3.8296e-01  5.8763e-02\n",
      "                ...                   ⋱                   ...                \n",
      "-5.1088e-02  1.8064e-01  1.0946e-01  ...   1.0074e-01  1.6894e-01  3.1154e-01\n",
      " 1.3909e-01  1.2871e-01  1.0447e-01  ...  -1.8229e-01  1.1846e-01  5.7177e-01\n",
      "-1.0478e-01  6.2195e-03 -1.0987e-02  ...  -8.7069e-02  7.1247e-02  6.3095e-02\n",
      "[torch.FloatTensor of size 44089x100]\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "pretrained = np.load('word_vectors.npy')\n",
    "embedding = nn.Embedding(num_embeddings=pretrained.shape[0], embedding_dim=pretrained.shape[1])\n",
    "print(embedding.weight)\n",
    "print(embedding.weight.requires_grad)\n",
    "embedding.weight = nn.Parameter(torch.Tensor(pretrained))\n",
    "print(embedding.weight)\n",
    "print(embedding.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.match = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.to_weight = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_state, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            (decoder current) hidden_state {Variable} -- (1, batch, hidden_size)\n",
    "            encoder_outputs {Variable} -- (batch, seq_len, hidden_size) \n",
    "        Returns:\n",
    "            Variable -- context vector of size batch_size x dim\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, feat_n = encoder_outputs.size()\n",
    "        # Resize hidden_state and copy it seq_len times, so that we can get its attention\n",
    "        # with each encoder_output\n",
    "        hidden_state = hidden_state.view(batch_size, 1, feat_n).repeat(1, seq_len, 1)\n",
    "\n",
    "        matching_inputs = torch.cat((encoder_outputs, hidden_state), 2).view(-1, 2*self.hidden_size)\n",
    "\n",
    "        attention_weights = self.to_weight(self.match(matching_inputs))\n",
    "        attention_weights = attention_weights.view(batch_size, seq_len)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return context\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, word_vec_filepath='word_vectors.npy', hidden_size=1024, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "    \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # load pretrained embedding\n",
    "        pretrained = np.load(word_vec_filepath)\n",
    "        self.vocab_size = pretrained.shape[0]\n",
    "        self.word_vec_dim = pretrained.shape[1]\n",
    "        \n",
    "        embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.word_vec_dim)\n",
    "        embedding.weight = nn.Parameter(torch.Tensor(pretrained)) # requires_grad == True\n",
    "        self.embedding = embedding # TODO: can let encoder and decoder share embeddings\n",
    "        \n",
    "        # feed word vector into encoder GRU\n",
    "        self.gru = nn.GRU(input_size=self.word_vec_dim, hidden_size=self.hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input): # input: (batch_size, sentence_length)\n",
    "        \"\"\"\n",
    "        param input: input sequence with shape (batch size, sequence_length)\n",
    "        return: gru output, hidden state\n",
    "        \"\"\"    \n",
    "        word_embeddings = self.embedding(input) # (batch_size, sentence_length, word_vec_dim)\n",
    "        top_layer_output, last_time_step_all_layers_output = self.gru(word_embeddings)\n",
    "        # top_layer_output: (seq_len, batch, hidden_size * num_directions)\n",
    "        # last_time_step_all_layers_output: (num_layers * num_directions, batch, hidden_size)\n",
    "        \n",
    "        return top_layer_output, last_time_step_all_layers_output\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, word_vec_filepath='word_vectors.npy', hidden_size=1024, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        # define hyper parameters\n",
    "        self.hidden_size = hidden_size # size of gru's Y and H\n",
    "        \n",
    "        # load pretrained embedding\n",
    "        pretrained = np.load(word_vec_filepath)\n",
    "        self.vocab_size = pretrained.shape[0]\n",
    "        self.word_vec_dim = pretrained.shape[1]\n",
    "        \n",
    "        embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.word_vec_dim)\n",
    "        embedding.weight = nn.Parameter(torch.Tensor(pretrained)) # requires_grad == True\n",
    "        self.embedding = embedding # TODO: can let encoder and decoder share embeddings\n",
    "\n",
    "        # gru input is word vector of prev_output_word (one hot), plus attention context vector\n",
    "        self.gru = nn.GRU(self.word_vec_dim+self.hidden_size, hidden_size=self.hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.attention = AttentionLayer(self.hidden_size)\n",
    "        # output is softmax over entire vocabulary\n",
    "        self.to_final_output = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, encoder_last_hidden_state, encoder_output, targets=None, mode='train', steps=None):\n",
    "        \"\"\"\n",
    "        :param encoder_last_hidden_state: (num_layers * num_directions, batch, hidden_size)\n",
    "        :param encoder_output: (batch, length_prev_sentences, hidden_size * num_directions)\n",
    "        :param targets: (batch, length_curr_sentences) target ground truth sentences\n",
    "        :param steps: just a parameter used for calculating scheduled sampling, unrelated to RNN time steps\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # parameters used in both train and inference stage\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        decoder_current_hidden_state = encoder_last_hidden_state # (encoder_num_layers * num_directions, batch, hidden_size)\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()  #<SOS> (batch x word index)\n",
    "        decoder_current_input_word = decoder_current_input_word.cuda() if torch.cuda.is_available() else decoder_current_input_word\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            raise NotImplementedError('Training target is None. Error location: RNNDecoder')\n",
    "        if steps is None:\n",
    "            raise NotImplementedError('steps is not specified. Error location: RNNDecoder -> steps')\n",
    "\n",
    "        # targets is only used for scheduled sampling, not used for calculating loss\n",
    "        targets = self.embedding(targets) # (batch, max_seq_len, embedding_size) embeddings of target labels of ground truth sentences\n",
    "        _, seq_len, _ = targets.size()\n",
    "\n",
    "        for i in range(seq_len-1): # only the length_curr_sentences-1 words will be the gru input, we exclude EOS token\n",
    "            \"\"\"\n",
    "            we implement the decoding procedure in a step by step fashion\n",
    "            so the seq_len is always 1\n",
    "            \"\"\"\n",
    "            threshold = self._get_teacher_learning_ratio(training_steps=steps)\n",
    "            \n",
    "            # target[:, i]: (batch, 1, embedding_size)\n",
    "            current_input_word = targets[:, i] if random.uniform(0.05, 0.995) > threshold \\\n",
    "                else self.embedding(decoder_current_input_word)\n",
    "            # current_input_word: (batch, 1, embedding_size)\n",
    "\n",
    "            # weighted sum of the encoder output w.r.t the current hidden state\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output) # (1, batch, hidden_size) (batch, seq_len, hidden_size) \n",
    "            # context: (batch, hidden_size)\n",
    "            gru_input = torch.cat([current_input_word.squeeze(1), context], dim=1).unsqueeze(1)\n",
    "            # gru_input: (batch, 1, embedding_size+hidden_size)\n",
    "\n",
    "            # only runs for one time step because sequence length is only 1\n",
    "            gru_output, decoder_current_hidden_state = self.gru(gru_input, decoder_current_hidden_state)\n",
    "            # gru_output (last time step): (batch, seq_length==1, hidden_size * num_directions)\n",
    "            # decoder_current_hidden_state (last layer): (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "            # project the dim of the gru output to match the final decoder output dim\n",
    "            # logprob = F.log_softmax(self.to_final_output(gru_output.squeeze(1)), dim=1)\n",
    "            logprob = self.to_final_output(gru_output.squeeze(1)) # logprob: (batch, vocab_size)\n",
    "            seq_logProb.append(logprob)\n",
    "\n",
    "            decoder_current_input_word = logprob.max(1)[1]\n",
    "            \n",
    "        # seq_logProb: list of [(batch, vocab_size), (batch, vocab_size)], len(list) == seq_len\n",
    "        seq_logProb = torch.stack(seq_logProb, dim=1)\n",
    "        # seq_logProb: (batch, seq_len, vocab_size)\n",
    "        \n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "\n",
    "        return seq_logProb, seq_predictions\n",
    "\n",
    "    # basically same as forward(), but without scheduled sampling\n",
    "    def infer(self, encoder_last_hidden_state, encoder_output):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        decoder_current_hidden_state = encoder_last_hidden_state # (encoder_num_layers * num_directions, batch, hidden_size)\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()  #<SOS> (batch x word index)\n",
    "        decoder_current_input_word = decoder_current_input_word.cuda() if torch.cuda.is_available() else decoder_current_input_word\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "\n",
    "        assumption_seq_len = 28 # run for fixed amount of time steps\n",
    "        for i in range(assumption_seq_len-1):\n",
    "\n",
    "            current_input_word = self.embedding(decoder_current_input_word)\n",
    "\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "\n",
    "            gru_input = torch.cat([current_input_word.squeeze(1), context], dim=1).unsqueeze(1)\n",
    "\n",
    "            gru_output, decoder_current_hidden_state = self.gru(gru_input, decoder_current_hidden_state)\n",
    "\n",
    "            logprob = self.to_final_output(gru_output.squeeze(1))\n",
    "            seq_logProb.append(logprob)\n",
    "\n",
    "            decoder_current_input_word = logprob.max(1)[1]\n",
    "\n",
    "        seq_logProb = torch.stack(seq_logProb, dim=1)\n",
    "\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "\n",
    "        return seq_logProb, seq_predictions\n",
    "\n",
    "\n",
    "    def _get_teacher_learning_ratio(self, training_steps):\n",
    "        return (expit(training_steps/40 +0.85))\n",
    "\n",
    "\n",
    "\n",
    "class VideoCaptionGenerator(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VideoCaptionGenerator, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, avi_feats, mode, target_sentences=None, steps=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            param avi_feats(Variable): size(batch size x 80 x 4096)\n",
    "            param target_sentences: ground truth for training, None for inference\n",
    "        Returns:\n",
    "            seq_logProb\n",
    "            seq_predictions\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_outputs, encoder_last_hidden_state = self.encoder(avi_feats)\n",
    "\n",
    "        if mode == 'train':\n",
    "            seq_logProb, seq_predictions = self.decoder(\n",
    "                encoder_last_hidden_state = encoder_last_hidden_state,\n",
    "                encoder_output = encoder_outputs,\n",
    "                targets = target_sentences,\n",
    "                mode = mode,\n",
    "                steps=steps\n",
    "            )\n",
    "\n",
    "        elif mode == 'inference':\n",
    "            seq_logProb, seq_predictions = self.decoder.infer(\n",
    "                encoder_last_hidden_state=encoder_last_hidden_state,\n",
    "                encoder_output=encoder_outputs,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise KeyError('mode is not valid')\n",
    "\n",
    "        return seq_logProb, seq_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my lil sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TrainingDataset, collate_fn\n",
    "from vocabulary import Vocabulary\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_data_path='data/clr_conversation.txt'\n",
    "helper = Vocabulary(training_data_path)\n",
    "dataset = TrainingDataset(training_data_path, helper)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN()\n",
    "decoder = DecoderRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(  0  ,.,.) = \n",
      "  4.0636e-03  1.4534e-02  4.3411e-02  ...   3.8113e-02  1.5125e-02  7.4324e-02\n",
      "  3.1995e-03 -2.8838e-02 -4.2324e-02  ...   5.1349e-02  2.0716e-02  5.2914e-02\n",
      "  3.4641e-03 -4.8222e-02 -5.4895e-02  ...   6.4168e-02  3.0389e-02  5.0499e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.5436e-02 -6.2421e-02 -5.0598e-02  ...   6.3670e-02  2.6074e-02  1.0488e-01\n",
      "  4.3065e-02 -8.5981e-02 -1.1462e-01  ...   4.3215e-02  1.1495e-01  1.3081e-01\n",
      "  3.2917e-02 -8.7677e-02 -2.0065e-01  ...   3.3761e-02  9.9658e-03  9.4516e-02\n",
      "\n",
      "(  1  ,.,.) = \n",
      " -9.3686e-03 -1.0405e-03  1.4614e-02  ...   8.5765e-03 -1.5679e-02  4.5429e-02\n",
      " -4.8277e-03 -5.7344e-03  3.2877e-03  ...   2.4134e-02 -2.3264e-02  3.6407e-02\n",
      " -1.1159e-02 -2.0141e-02 -2.1574e-02  ...   3.7705e-02 -3.2930e-02  3.1977e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.8472e-02 -7.0078e-02  2.3107e-02  ...  -4.5401e-03 -3.1738e-02  4.1753e-02\n",
      " -1.3478e-01 -1.8731e-01  1.1766e-01  ...   9.8724e-02 -6.5632e-02 -1.4088e-02\n",
      " -1.2424e-01 -2.5603e-01  1.1434e-01  ...   3.3677e-02 -9.9327e-02  1.0070e-01\n",
      "\n",
      "(  2  ,.,.) = \n",
      "  7.3992e-03 -2.6735e-02  1.6699e-02  ...   4.3401e-02 -4.2059e-02  2.2064e-02\n",
      "  1.4835e-02 -4.4425e-02 -7.7748e-03  ...   5.5703e-02 -5.9305e-02  1.4618e-02\n",
      "  3.8559e-03 -4.1279e-02 -1.7120e-02  ...   7.2514e-02 -5.1767e-02  9.2498e-03\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.0140e-03 -3.3939e-02 -2.4445e-02  ...   7.2721e-02 -3.7288e-02 -5.4983e-03\n",
      "  3.0105e-02 -7.5799e-02  2.8556e-02  ...   2.4618e-02  2.0457e-02 -1.5079e-01\n",
      " -1.1548e-03 -3.3323e-02  7.0193e-02  ...   2.6904e-02 -3.0191e-02 -4.2053e-02\n",
      " ...  \n",
      "\n",
      "( 125 ,.,.) = \n",
      "  1.6723e-02 -3.4287e-03  2.4606e-02  ...   4.8379e-02 -6.8489e-02  3.0251e-02\n",
      "  1.6201e-02 -7.1464e-03 -6.2397e-03  ...   5.7036e-02 -4.6542e-02  2.7684e-02\n",
      " -4.6868e-04 -4.1190e-02 -4.1866e-02  ...   1.4088e-01 -1.5510e-02  4.4701e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -2.6814e-03 -3.2107e-02 -1.6343e-02  ...   4.7857e-02 -5.2686e-02  1.9207e-03\n",
      " -3.7008e-03 -2.9852e-02 -9.9262e-03  ...   5.0548e-02 -3.7326e-02 -1.3374e-02\n",
      " -4.1494e-03 -2.8722e-02 -9.6197e-03  ...   5.5042e-02 -3.0437e-02 -2.1791e-02\n",
      "\n",
      "( 126 ,.,.) = \n",
      " -9.5262e-03 -3.1126e-03  4.6577e-02  ...   1.6463e-02 -2.9844e-02  3.8584e-02\n",
      " -9.4499e-03 -1.3971e-02  7.2891e-03  ...   2.8233e-02 -2.7795e-02  3.5271e-02\n",
      " -5.9484e-03 -1.8467e-02  1.3036e-04  ...   3.1040e-02 -2.2982e-02  2.9365e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  2.9070e-03 -4.6410e-02 -5.1890e-02  ...   5.7497e-02 -4.8182e-02  2.8550e-02\n",
      " -3.2173e-03 -5.3399e-02 -3.2447e-02  ...   5.1034e-02 -2.1822e-02  1.4122e-02\n",
      " -8.9574e-03 -5.4532e-02 -2.3473e-02  ...   4.9049e-02 -1.0288e-02  4.9388e-03\n",
      "\n",
      "( 127 ,.,.) = \n",
      "  8.0014e-03 -1.4447e-02  2.3985e-02  ...   2.3473e-02 -3.3130e-02  3.9606e-02\n",
      "  2.3637e-02 -7.9897e-02 -1.8011e-02  ...   8.4259e-03 -5.0735e-02  7.5917e-02\n",
      "  2.1417e-02 -4.8865e-02  2.9577e-03  ...   2.0247e-02 -4.3017e-02  4.6892e-02\n",
      "                 ...                   ⋱                   ...                \n",
      " -1.1364e-02 -8.4135e-02  4.1275e-02  ...   5.5638e-02  1.1827e-02  1.7736e-02\n",
      " -1.6678e-02 -5.9576e-02  2.0359e-02  ...   5.5493e-02  4.2957e-03 -2.9092e-03\n",
      " -2.0410e-02 -4.9890e-02  7.6199e-03  ...   5.7523e-02 -1.3607e-03 -1.1217e-02\n",
      "[torch.FloatTensor of size 128x15x44089]\n",
      "\n",
      "\n",
      "Variable containing:\n",
      " 17536  43242   6610  ...   41198  20898   3070\n",
      " 41139  25451  41139  ...   14200   9687   9477\n",
      " 26424   2902  24768  ...    6621  42762  11698\n",
      "        ...            ⋱           ...         \n",
      " 27081   4019   2445  ...   21498  40407  40407\n",
      " 38146  41139  19508  ...     872  23365  23365\n",
      "  5932  13936  35490  ...    6599  42490  24257\n",
      "[torch.LongTensor of size 128x15]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.train()\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    padded_prev_sentences, padded_curr_sentences, lengths_curr_sentences = batch\n",
    "    padded_prev_sentences, padded_curr_sentences = Variable(padded_prev_sentences), Variable(padded_curr_sentences)\n",
    "    \n",
    "    top_layer_output, last_time_step_all_layers_output = encoder(padded_prev_sentences)\n",
    "    \n",
    "    seq_logProb, seq_predictions = decoder(\n",
    "        encoder_last_hidden_state = last_time_step_all_layers_output,\n",
    "        encoder_output = top_layer_output,\n",
    "        targets = padded_curr_sentences,\n",
    "        steps=1\n",
    "    )\n",
    "    print(seq_logProb)\n",
    "    print()\n",
    "    print(seq_predictions)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initalizing vocabulary...\n",
      "Building mapping...\n",
      "Parsing training data to dataset...\n",
      "epoch: 1\n",
      "batch no: 0\n",
      "\n",
      "\n",
      "Columns 0 to 10 \n",
      "     1   1861   3430   3200    926     36   5360    926     36      2      0\n",
      "     1    110    389  26774     32   1107    319      9     32  12447     25\n",
      "     1    105     32  30827    489    110  29918    343    249   2010   2463\n",
      "\n",
      "Columns 11 to 12 \n",
      "     0      0\n",
      "  5025      2\n",
      "     2      0\n",
      "[torch.LongTensor of size 3x13]\n",
      "\n",
      "\n",
      "[10, 13, 12]\n",
      "\n",
      "\n",
      "     1    660    201   4941   1208    350    139    574    794    515      2\n",
      "     1    951      9    256   7091    243    690    319   2794     95      2\n",
      "     1    619     72    422    187    487  16316    200      2      0      0\n",
      "[torch.LongTensor of size 3x11]\n",
      "\n",
      "\n",
      "[11, 11, 9]\n",
      "\n",
      "['<SOS>', '回去', '睡覺', '派對', '結束', '了', '表演', '結束', '了', '<EOS>', '<PAD>', '<PAD>', '<PAD>']\n",
      "['<SOS>', '沒有', '任何', '破門而入', '的', '痕跡', '但是', '我', '的', '抽屜', '和', '物品', '<EOS>']\n",
      "['<SOS>', '他', '的', '軟盤', '上', '沒有', '寫字', '而且', '全都', '一起', '帶著', '<EOS>', '<PAD>']\n",
      "\n",
      "['<SOS>', '那', '不是', '實際', '情況', '你', '可', '不能', '告訴', '她', '<EOS>']\n",
      "['<SOS>', '其實', '我', '沒', '丟', '什麼', '東西', '但是', '確實', '有人', '<EOS>']\n",
      "['<SOS>', '再說', '這個', '地方', '是', '間', '密室', '吧', '<EOS>', '<PAD>', '<PAD>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import logging\n",
    "    logger.setLevel(logging.INFO)\n",
    "    from vocabulary import Vocabulary\n",
    "\n",
    "    json_file = 'data/testing_label.json'\n",
    "    numpy_file = 'data/testing_data/feat'\n",
    "\n",
    "    helper = Vocabulary(json_file, min_word_count=5)\n",
    "\n",
    "\n",
    "\n",
    "    input_data = Variable(torch.randn(3, 80, 4096).view(-1, 80, 4096))\n",
    "\n",
    "    encoder = EncoderRNN(input_size=4096, hidden_size=1000)\n",
    "    decoder = DecoderRNN(hidden_size=1000, output_size=1700, vocab_size=1700, word_dim=128, helper=helper)\n",
    "\n",
    "    model = VideoCaptionGenerator(encoder=encoder, decoder=decoder)\n",
    "\n",
    "    ground_truth = Variable(torch.rand(3, 27)).long()\n",
    "\n",
    "    for step in range(50, 100):\n",
    "        seq_prob, seq_predict = model(avi_feats=input_data, mode='train', target_sentences=ground_truth, steps=step)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(seq_prob.size())\n",
    "            print(seq_predict.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 2. 3. 4.]\n",
      " [1. 2. 3. 4. 5. 6. 7.]\n",
      " [0. 0. 0. 0. 0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "prev_sentences = [[1,2,3,4], [1,2,3,4,5,6,7], [1,2]]\n",
    "lengths_prev_sentences = [len(sentence) for sentence in prev_sentences]\n",
    "padded_prev_sentences = np.zeros((len(prev_sentences), max(lengths_prev_sentences)))\n",
    "for i, sentence in enumerate(prev_sentences):\n",
    "    end = lengths_prev_sentences[i]\n",
    "    padded_prev_sentences[i, -end:] = sentence[:end]\n",
    "\n",
    "print(padded_prev_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
